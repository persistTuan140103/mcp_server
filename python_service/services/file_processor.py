import asyncio
import os
from pathlib import Path
import uuid
from langchain_unstructured import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from typing import List, Optional, Dict, AsyncGenerator
import logging
from bs4 import BeautifulSoup
from fastapi import HTTPException
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed
from unstructured.staging.base import elements_from_base64_gzipped_json
from unstructured_client import RetryConfig, UnstructuredClient
from unstructured_client.utils.retries import BackoffStrategy
from models.DocxModel import DocxModel
from models.PdfModel import PdfModel

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    def __init__(self, 
                 unstructured_api_url: str = "http://localhost:8000",
                 api_key: Optional[str] = None):
        """
        Kh·ªüi t·∫°o processor v·ªõi Unstructured Docker API
        
        Args:
            unstructured_api_url: URL c·ªßa Unstructured API (Docker)
            api_key: API key n·∫øu c·∫ßn (c√≥ th·ªÉ None cho local)
        """
        self.api_url = unstructured_api_url
        self.api_key = api_key or ""
        
        self.client = UnstructuredClient(
            server_url=unstructured_api_url,
            api_key_auth=self.api_key,
            timeout_ms=30000, # 30s
            retry_config=RetryConfig(
                retry_connection_errors=True,
                strategy="backoff",
                backoff=BackoffStrategy(
                    initial_interval=500,
                    max_interval=60000, # 60s
                    exponent=1.5,
                    max_elapsed_time=900000 # 15m
                )
            )
        )
        
        
    @retry(
        wait=wait_fixed(10),  # ƒê·ª£i 10 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
        stop=stop_after_attempt(3), # Th·ª≠ l·∫°i t·ªëi ƒëa 3 l·∫ßn
        retry=retry_if_exception_type(asyncio.TimeoutError) | retry_if_exception_type(Exception) # C√≥ th·ªÉ t√πy ch·ªânh lo·∫°i l·ªói
    )
    async def process_pdf_document(self, 
                           model: PdfModel) -> AsyncGenerator[Document, None]:
        """
        X·ª≠ l√Ω file PDF v·ªõi OCR v√† extract tables/images
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n t·ªõi file PDF
            chunk_size: K√≠ch th∆∞·ªõc chunk t·ªëi ƒëa
            chunk_overlap: ƒê·ªô overlap gi·ªØa c√°c chunk
            enable_ocr: B·∫≠t OCR cho images trong PDF
            extract_tables: Extract tables t·ª´ PDF
            languages: Danh s√°ch ng√¥n ng·ªØ cho OCR
        
        Returns:
            List c√°c Document ƒë√£ ƒë∆∞·ª£c chunk
        """
        try:
            logger.info(f"ƒêang x·ª≠ l√Ω PDF: {model.file_name}")
            
            # C·∫•u h√¨nh t·ªëi ∆∞u cho PDF
            loader = UnstructuredLoader(
                kwargs={
                    "filename": model.file_name,
                    "split_pdf_concurrency_level": 10
                },
                file=model.file,
                timeout_ms=10000,
                client=self.client,
                partition_via_api=True,
                
                # === STRATEGY T·ªêI ∆ØU CHO PDF ===
                strategy="hi_res",                    # Ch·∫•t l∆∞·ª£ng cao cho PDF
                hi_res_model_name="yolox",   # Model layout t·ªët nh·∫•t
                
                # === OCR V√Ä NG√îN NG·ªÆ ===
                languages=model.languages,                  # H·ªó tr·ª£ ti·∫øng Vi·ªát + English
                coordinates=True,                     # L·∫•y t·ªça ƒë·ªô ƒë·ªÉ debug layout
                
                # === PDF SPECIFIC ===
                # pdf_infer_table_structure=model.extract_tables,  # Infer c·∫•u tr√∫c b·∫£ng
                extract_image_block_types=["Image", "Table"] if model.enable_ocr else ["Table"],
                skip_infer_table_types=[],            # Kh√¥ng skip table types
                include_page_breaks=True,             # Gi·ªØ page breaks
                
                # === CHUNKING BY_TITLE CHO PDF ===
                chunking_strategy="by_title",         # Chunk theo c·∫•u tr√∫c semantic
                max_characters=model.chunk_size,            # Hard limit
                new_after_n_chars=model.chunk_size - model.chunk_overlap,  # Soft limit
                combine_under_n_chars=150,            # K·∫øt h·ª£p chunks nh·ªè (PDF th∆∞·ªùng c√≥ nhi·ªÅu ƒëo·∫°n ng·∫Øn)
                overlap=model.chunk_overlap,                # Overlap ƒë·ªÉ ƒë·∫£m b·∫£o context
                overlap_all=True,                     # Apply overlap cho t·∫•t c·∫£
                multipage_sections=True,              # Cho ph√©p sections span nhi·ªÅu trang
                
                # === METADATA V√Ä DEBUG ===
                unique_element_ids=True,              # Unique IDs cho debug
                include_orig_elements=True,           # Bao g·ªìm original elements
                
                # === OUTPUT ===
                output_format="application/json",
                encoding="utf-8"
            )
            
            # Load documents
            index = 0
            metadata_main :List[Dict] = []
            
            # Th√™m metadata ƒë·∫∑c bi·ªát cho PDF
            async for doc in loader.alazy_load():
                if("text_as_html" in doc.metadata):
                    text = self.parse_html_table_to_text(doc.metadata["text_as_html"])
                    doc.page_content += text
                decoded_elements_serializable = self.__decode_orig_elements(doc.metadata["orig_elements"])
                    
                # res = self.__create_main_title(metadata_main, decoded_elements_serializable)                           
                # main_title = ". ".join([item["text"] for item in res])
                
                doc.metadata.pop("orig_elements")
                doc.id = str(uuid.uuid4())
                doc.metadata.update({
                    'filename': model.file_name,
                    'file_type': 'pdf',
                    'chunk_index': index,
                    'chunk_size': len(doc.page_content),
                    'processing_strategy': 'hi_res',
                    'ocr_enabled': model.enable_ocr,
                    'tables_extracted': model.extract_tables,
                    'languages': model.languages,
                    # "content_title_of_chunk": main_title,
                    "orig_elements_decoded": decoded_elements_serializable
                })
                yield doc
                index += 1
            
            logger.info(f"PDF processed: {index} chunks t·ª´ {model.file_name}")
            # return documents
            
        except Exception as e:
            logger.error(f"L·ªói x·ª≠ l√Ω PDF {model.file_name}: {str(e)}")
            raise e

    async def process_docx_document(self, 
                            model: DocxModel) -> AsyncGenerator[Document, None]:
        """
        X·ª≠ l√Ω file DOCX v·ªõi t·ªëi ∆∞u cho c·∫•u tr√∫c vƒÉn b·∫£n
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n t·ªõi file DOCX
            chunk_size: K√≠ch th∆∞·ªõc chunk t·ªëi ƒëa
            chunk_overlap: ƒê·ªô overlap gi·ªØa c√°c chunk
            preserve_formatting: Gi·ªØ nguy√™n formatting
        
        Returns:
            List c√°c Document ƒë√£ ƒë∆∞·ª£c chunk
        """
        try:
            logger.info(f"ƒêang x·ª≠ l√Ω DOCX: {model.file_name}")
            
            # C·∫•u h√¨nh t·ªëi ∆∞u cho DOCX
            loader = UnstructuredLoader(
                file=model.file,
                filename=model.file_name,
                client=self.client,
                partition_via_api=True,
                model="element",
               
                # === STRATEGY T·ªêI ∆ØU CHO DOCX ===
                strategy="fast",                      # DOCX kh√¥ng c·∫ßn hi_res nh∆∞ PDF
                
                # === DOCX SPECIFIC ===
                include_page_breaks=model.preserve_formatting,  # Gi·ªØ page breaks n·∫øu c·∫ßn
                xml_keep_tags=False,                  # Kh√¥ng gi·ªØ XML tags
                
                # === CHUNKING BY_TITLE - T·ªêI ∆ØU CHO DOCX ===
                chunking_strategy="by_title",         # Chunk theo ti√™u ƒë·ªÅ/c·∫•u tr√∫c
                max_characters=model.chunk_size,            # Hard limit nh·ªè h∆°n PDF
                new_after_n_chars=model.chunk_size - model.chunk_overlap,  # Soft limit
                combine_under_n_chars=100,            # DOCX c√≥ c·∫•u tr√∫c t·ªët h∆°n PDF
                overlap=model.chunk_overlap,                # Overlap v·ª´a ph·∫£i
                overlap_all=True,                     # Apply overlap
                multipage_sections=True,              # Sections c√≥ th·ªÉ span pages
                
                # === METADATA ===
                unique_element_ids=True,
                include_orig_elements=True,           # Original elements cho debug
                
                # === OUTPUT ===
                output_format="application/json",
                encoding="utf-8"
            )
            
            # Load documents
            
            
            # async for doc in loader.alazy_load():
            #     yield doc
                
            
            # Th√™m metadata ƒë·∫∑c bi·ªát cho DOCX
            # breakpoint()
            metadata_main :List[Dict] = []
            index = 0
            async for doc in loader.alazy_load():
                
                main_title = ""
                if("text_as_html" in doc.metadata):
                    text = self.parse_html_table_to_text(doc.metadata["text_as_html"])
                    doc.page_content += text
                    
                
                decoded_elements_serializable = self.__decode_orig_elements(doc.metadata["orig_elements"])
                    
                res = self.__create_main_title(metadata_main, decoded_elements_serializable)                           
                
                metadata_main = res               
                    
                main_title = ". ".join([item["text"] for item in res])
                
                doc.metadata.pop("orig_elements")
                doc.id = str(uuid.uuid4())
                doc.metadata.update({
                    'filename': model.file_name,
                    'file_type': 'docx',
                    'chunk_index': index,
                    'chunk_size': len(doc.page_content),
                    'processing_strategy': 'fast',
                    'preserve_formatting': model.preserve_formatting,
                    'chunking_strategy': 'by_title',
                    "content_title_of_chunk": main_title,
                    "orig_elements_decoded": decoded_elements_serializable
                })
                yield doc
                index += 1
            logger.info(f"DOCX processed: {index} chunks t·ª´ {model.file_name}")
            # return documents
            
        except Exception as e:
            error_message = str(e)
            logger.error(f"L·ªói x·ª≠ l√Ω DOCX {model.file_name}: {error_message}")
            if('404' in error_message):
                raise HTTPException(
                    status_code=404,
                    detail=f"Url Unstructured API is not available {self.api_url}"
                )
            raise HTTPException(
                status_code=500,
                detail=f"Error processing DOCX file: {str(e)}"
            )

    async def post_process_chunks(self, 
                          documents: List[Document],
                          target_size: int = 600,
                          overlap: int = 100) -> List[Document]:
        """
        Post-process ƒë·ªÉ chia nh·ªè th√™m c√°c chunks l·ªõn
        
        Args:
            documents: Documents t·ª´ Unstructured
            target_size: K√≠ch th∆∞·ªõc m·ª•c ti√™u cu·ªëi c√πng
            overlap: Overlap cho post-processing
        
        Returns:
            List documents ƒë√£ ƒë∆∞·ª£c post-process
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=target_size,
            chunk_overlap=overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        final_documents = []
        
        for doc in documents:
            if len(doc.page_content) > target_size:
                # Split th√†nh sub-chunks
                sub_chunks = text_splitter.split_documents([doc])
                
                for i, sub_chunk in enumerate(sub_chunks):
                    sub_chunk.metadata.update({
                        'parent_chunk_index': doc.metadata.get('chunk_index', 0),
                        'sub_chunk_index': i,
                        'is_sub_chunk': True,
                        'post_processed': True
                    })
                
                final_documents.extend(sub_chunks)
            else:
                doc.metadata['post_processed'] = False
                final_documents.append(doc)
        
        return final_documents

    def parse_html_table_to_text(self, html_table: str) -> str:
        """
        Chuy·ªÉn b·∫£ng HTML (text_as_html t·ª´ Unstructured) th√†nh d·∫°ng vƒÉn b·∫£n d·ªÖ hi·ªÉu.
        Tr·∫£ v·ªÅ m·ªôt chu·ªói vƒÉn b·∫£n m√¥ t·∫£ t·ª´ng d√≤ng theo ƒë·ªãnh d·∫°ng "C·ªôt1: Gi√° tr·ªã1, C·ªôt2: Gi√° tr·ªã2"
        """
        soup = BeautifulSoup(html_table, 'html.parser')
        table = soup.find('table')
        if not table:
            return ""

        # L·∫•y header n·∫øu c√≥
        headers = []
        header_row = table.find('tr')
        if header_row:
            ths = header_row.find_all(['th', 'td'])
            headers = [th.get_text(strip=True) for th in ths]

        rows_text = []
        for row in table.find_all('tr')[1:]:  # B·ªè header
            cells = row.find_all(['td', 'th'])  # ƒë·ªÉ ph√≤ng b·∫£ng kh√¥ng chu·∫©n
            values = [cell.get_text(strip=True) for cell in cells]

            # B·ªï sung header n·∫øu b·ªã thi·∫øu
            if len(headers) < len(values):
                headers += [f"C·ªôt{i+1}" for i in range(len(headers), len(values))]

            row_data = [f"{headers[i]}: {values[i]}" for i in range(min(len(headers), len(values)))]
            rows_text.append(", ".join(row_data))

        return ".".join(rows_text)

    def __decode_orig_elements(self, orig_elements: str) -> List[Dict]:
        """
        Gi·∫£i m√£ v√† chuy·ªÉn ƒë·ªïi orig_elements th√†nh d·∫°ng dictionary
        """
        try:
            orig_elements = elements_from_base64_gzipped_json(orig_elements)
            # Chuy·ªÉn ƒë·ªïi m·ªói element th√†nh dictionary
            decoded_elements_serializable = []
            for element in orig_elements:
                if hasattr(element, 'to_dict'): # Ki·ªÉm tra xem element c√≥ ph∆∞∆°ng th·ª©c to_dict kh√¥ng
                    decoded_elements_serializable.append(element.to_dict())
                else:
                    # N·∫øu kh√¥ng c√≥ to_dict, th·ª≠ chuy·ªÉn ƒë·ªïi m·ªôt c√°ch th·ªß c√¥ng ho·∫∑c b·ªè qua
                    logger.warning(f"Element type {type(element)} does not have to_dict() method. Skipping serialization.")
                    # Ho·∫∑c b·∫°n c√≥ th·ªÉ th·ª≠ bi·ªÉu di·ªÖn ƒë∆°n gi·∫£n h∆°n n·∫øu c·∫ßn
                    decoded_elements_serializable.append(str(element))
            
            return decoded_elements_serializable
        except Exception as e:
            logger.error(f"Failed to decode or serialize orig_elements: {e}")
            return [] # G√°n r·ªóng ƒë·ªÉ tr√°nh l·ªói

    def __create_main_title(self, source_main_title: List[dict], 
                            source_metadata: List[dict]) -> List[dict]:
        result = source_main_title.copy()
        list_main_title = ["Title", "ListItem"]
        
        def get_index(source_main_title: List[dict], target: dict):
            for i, item in enumerate(source_main_title):
                if(item["type"] == target["type"] and item["category_depth"] == target["category_depth"]):
                    return i
            return -1
        
        for metadata in source_metadata:
            # breakpoint()
                
            if(metadata["type"] in list_main_title):
                if(metadata["type"] == "Title" and metadata["metadata"]["category_depth"] == 0):
                    result.clear()
                if(len(result) == 0):
                    result.append({
                        "type": metadata["type"],
                        "category_depth": metadata["metadata"]["category_depth"],
                        "text": metadata["text"]
                    })
                else:
                    tmp = {
                        "type": metadata["type"],
                        "category_depth": metadata["metadata"]["category_depth"],
                        "text": metadata["text"]
                    }
                    index = get_index(result, tmp)
                    if(index == -1):
                        result.append(tmp)
                    else:
                        result = result[:(index)]
                        result.append(tmp)
                    
        return result
                            

# Utility functions
def analyze_processing_results(results: Dict[str, List[Document]]):
    """Ph√¢n t√≠ch k·∫øt qu·∫£ x·ª≠ l√Ω batch"""
    
    print("=== PH√ÇN T√çCH K·∫æT QU·∫¢ X·ª¨ L√ù ===")
    total_chunks = 0
    
    for file_path, documents in results.items():
        file_name = os.path.basename(file_path)
        file_type = Path(file_path).suffix.lower()
        
        if documents:
            chunk_sizes = [len(doc.page_content) for doc in documents]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            
            print(f"\nüìÑ {file_name} ({file_type.upper()})")
            print(f"   Chunks: {len(documents)}")
            print(f"   Avg size: {avg_size:.0f} chars")
            print(f"   Size range: {min(chunk_sizes)}-{max(chunk_sizes)} chars")
            
            # Hi·ªÉn th·ªã sample chunk
            if documents:
                sample = documents[0]
                strategy = sample.metadata.get('processing_strategy', 'unknown')
                print(f"   Strategy: {strategy}")
                print(f"   Sample: {sample.page_content[:100]}...")
            
            total_chunks += len(documents)
        else:
            print(f"\n {file_name} ({file_type.upper()}): L·ªói x·ª≠ l√Ω")
    
    print(f"\n T·ªïng c·ªông: {total_chunks} chunks t·ª´ {len(results)} files")
    
